{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z933R9wuZ4z"
   },
   "outputs": [],
   "source": [
    "# Installing the Portkey AI gateway developed by the Portkey team\n",
    "!pip install portkey-ai -U\n",
    "!portkey --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import portkey as pk\n",
    "from portkey import Config, LLMOptions\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter the password on the prompt window.\n",
    "API_KEY = getpass(\"Enter you PORTKEY_API_KEY \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the API key\n",
    "pk.api_key = API_KEY\n",
    "\n",
    "# NOTE: For adding custom url, uncomment this line and add your custom url.\n",
    "# pk.base_url = \"\"\n",
    "\n",
    "# Setting the config for portkey\n",
    "pk.config = Config(\n",
    "    mode=\"single\", llms=LLMOptions(virtual_key=\"open-ai-key-66a67d\", provider=\"openai\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic example\n",
    "\n",
    "response = pk.Completions.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Translate the following English text to French: 'Hello, how are you?'\",\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Using optional params\n",
    "\n",
    "response2 = pk.Completions.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Summarize the key points of Albert Einstein's theory of relativity.\",\n",
    "    max_tokens=100,  # Limit the generated text to 100 tokens\n",
    "    temperature=0.7,  # Use a lower temperature for deterministic output\n",
    "    n=3,  # Generate 3 completions for the same prompt\n",
    "    presence_penalty=0.5,  # Penalize new tokens based on their presence in the text\n",
    "    frequency_penalty=0.2,  # Penalize new tokens based on their frequency in the text\n",
    "    logit_bias={\"50256\": -100},  # Prevent a specific token from being generated\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(response2.choices):\n",
    "    print(f\"\\nCompletion {i + 1}: {choice.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Streaming results\n",
    "\n",
    "response3 = pk.Completions.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Once upon a time\",\n",
    "    max_tokens=50,\n",
    "    stream=True,  # Stream back partial progress\n",
    ")\n",
    "\n",
    "for event in response3:\n",
    "    print(event.choices[0].text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Echo back the prompt\n",
    "response4 = pk.Completions.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Echo this prompt: 'Hello, World!'\",\n",
    "    echo=True,  # Echo back the prompt in addition to the completion\n",
    ")\n",
    "print(response4.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Best-of completions\n",
    "response5 = pk.Completions.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Complete this sentence: 'The quick brown fox'\",\n",
    "    best_of=3,  # Generate 3 completions server-side and return the best one\n",
    ")\n",
    "print(response5.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Using logprobs parameter\n",
    "response6 = pk.Completions.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Generate text about the solar system.\",\n",
    "    max_tokens=50,\n",
    "    logprobs=5,  # Include log probabilities for the top 5 tokens\n",
    ")\n",
    "\n",
    "generated_text = response6.choices[0].text\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "# Print log probabilities for the top 5 tokens\n",
    "logprobs = response6.choices[0].logprobs  # Log probabilities for the sampled tokens\n",
    "print(\"\\nLog Probabilities for Top 5 Tokens:\")\n",
    "for token, logprob in zip(logprobs[\"tokens\"], logprobs[\"token_logprobs\"]):\n",
    "    print(f\"Token: {token}, Log Probability: {logprob}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
